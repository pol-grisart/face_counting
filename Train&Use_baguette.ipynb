{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanislas Deneuville - Emmanuel Ferrandi - Pol Grisart - Marine MÃ©dard\n",
    "# Project of data science :  Face recognition in a video and counting\n",
    "16/11/2018\n",
    "\n",
    "## Part II : Train&Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import keras\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__split__ : split the set into two well mixed set \n",
    "\n",
    "\n",
    "In : \n",
    "* data : list of all the images \n",
    "\n",
    "Out : \n",
    "* train_set : list of the images in the training set representing 66% of data\n",
    "* test_set : list of the images in the test set representing the other 34% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_RATIO = 0.66\n",
    "def split(data:list, train_test_ratio:int=TRAIN_TEST_RATIO, random_split=True) :\n",
    "    if random_split:\n",
    "        # Shuffle\n",
    "        random.shuffle(data)\n",
    "    \n",
    "    # Split data\n",
    "    cut_index = round(len(data) * train_test_ratio) \n",
    "    train_set = data[:cut_index]\n",
    "    test_set = data[cut_index:]\n",
    "    return(train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CATEGORY = 10\n",
    "\n",
    "def equalize(subsets):\n",
    "    print(\"Equalizing subset\")\n",
    "    for nb_face,subset in enumerate(subsets):\n",
    "        print(\"{} faces set contains {} images\".format(nb_face, len(subset)))\n",
    "    minimum = max(100,min([len(subset) for subset in subsets]))\n",
    "    cutted_subsets = [subset[:minimum] for subset in subsets]\n",
    "    return cutted_subsets\n",
    "\n",
    "def load_and_split():\n",
    "    subsets = [[] for k in range(MAX_CATEGORY+1)]\n",
    "    for nb_face in range(MAX_CATEGORY+1):\n",
    "        # Y value of these alements\n",
    "        categorical_y = np.zeros((1, MAX_CATEGORY+1))\n",
    "        categorical_y[0, nb_face] = 1\n",
    "        \n",
    "        folder_path = os.path.join(\"train_set\", str(nb_face))\n",
    "        for filename in os.listdir(folder_path):\n",
    "            # Filter non image files\n",
    "            if \".jpeg\" in filename or \".png\" in filename or \".jpg\" in filename:\n",
    "                x = plt.imread(os.path.join(folder_path, filename)).reshape((1, 50,50))\n",
    "                y = categorical_y\n",
    "                xy = (x,y)\n",
    "                subsets[nb_face].append(xy)\n",
    "    \n",
    "    # Equalize to have the same number of each Y value\n",
    "    equalized_subsets = equalize(subsets)\n",
    "    \n",
    "    # Split data respecting equalization\n",
    "    train_set, test_set = [], []\n",
    "    for subset in equalized_subsets:\n",
    "        add_train_set, add_test_set = split(subset)\n",
    "        train_set = train_set + add_train_set\n",
    "        test_set = test_set + add_test_set\n",
    "    \n",
    "    # Shuffle\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    print(\"Train size = {}, test size = {}\".format(len(train_set), len(test_set)))\n",
    "    \n",
    "    return train_set, test_set\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__train_neuural_network__ : function that creates a neural network and trains it with the train_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyPlotMemory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.plot_y = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.plot_y.append(logs.get('acc'))\n",
    "        \n",
    "    def plot(self):\n",
    "        plt.plot([k+1 for k in range(len(self.plot_y))], self.plot_y)\n",
    "        \n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.show()\n",
    "        \n",
    "plot_callback = AccuracyPlotMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    # Simple model\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Add neurone layers\n",
    "    #model.add(keras.layers.Dense(units=500, activation='relu', input_dim=50*50))\n",
    "    #model.add(keras.layers.Dense(units=100, activation='relu'))\n",
    "    #model.add(keras.layers.Dense(units=40, activation='relu'))\n",
    "    #model.add(keras.layers.Dense(units=15, activation='relu'))\n",
    "    \n",
    "    # Convolutionnal model\n",
    "    model.add(keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(50,50, 1)))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(keras.layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    # End with a classic model\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1000, activation='relu'))\n",
    "    model.add(keras.layers.Dense(MAX_CATEGORY+1, activation='softmax'))\n",
    "    \n",
    "    # Learning process\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "def train_neural_network(train_set, test_set=None):\n",
    "    print(\"Generating model\")\n",
    "    model = generate_model()\n",
    "    \n",
    "    x_train = np.concatenate([x for x,y in train_set]).reshape(len(train_set), 50, 50, 1)\n",
    "    y_train = np.concatenate([y for x,y in train_set])\n",
    "    print(\"X train size = {}, y train  size = {}\".format(x_train.shape, y_train.shape))\n",
    "    \n",
    "    xy_test = None\n",
    "    if test_set is not None:\n",
    "        x_test = np.concatenate([x for x,y in test_set]).reshape(len(test_set), 50, 50, 1)\n",
    "        y_test = np.concatenate([y for x,y in test_set])\n",
    "        xy_test = x_test, y_test\n",
    "    \n",
    "    print(\"Start training\")\n",
    "    model.fit(x_train, y_train, epochs=30, batch_size=32, callbacks=[plot_callback], validation_data=xy_test)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__evaluate_performance__ : function that applys the neural network on the images in the test_set and compare with the real number of faces in these images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(model, test_set):\n",
    "    \n",
    "    x_test = np.concatenate([x for x,y in test_set]).reshape(len(test_set), 50, 50, 1)\n",
    "    y_test = np.concatenate([y for x,y in test_set])\n",
    "    plot_callback.plot()\n",
    "    \n",
    "    loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\n",
      "Train size = (157, 2515), test size = (76, 2515)\n",
      "Train\n",
      "Generating model\n",
      "x train [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Start training\n",
      "Epoch 1/5\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 6.9554 - acc: 0.1911\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5.1468 - acc: 0.4459\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2.8485 - acc: 0.6688\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2.2530 - acc: 0.8025\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.9981 - acc: 0.8344\n",
      "Evaluate\n",
      "76/76 [==============================] - 0s 305us/step\n"
     ]
    }
   ],
   "source": [
    "print(\"Split\")\n",
    "train_set, test_set = load_and_split()\n",
    "\n",
    "print(\"Train\")\n",
    "model = train_neural_network(train_set)\n",
    "\n",
    "print(\"Evaluate\")\n",
    "evaluate_performance(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "def manual_evaluation(model, test_et):\n",
    "    x_test = np.concatenate([x for x,y in test_set]).reshape(len(test_set), 50, 50, 1)\n",
    "    y_test = np.concatenate([y for x,y in test_set])\n",
    "    outs = model.predict(x_test)\n",
    "    \n",
    "    result = [[] for k in range(MAX_CATEGORY + 1)]\n",
    "    \n",
    "    square_error = 0\n",
    "    perfect_accuracy = 0\n",
    "    count = 0\n",
    "    \n",
    "    for y_out, y_expected in zip(outs, y_test):\n",
    "        #print(\">>\",y_out,\"/expects/\",y_expected)\n",
    "        clean_expected_out = 0\n",
    "        while not y_expected[clean_expected_out] == 1:\n",
    "            clean_expected_out += 1\n",
    "        clean_out = y_out.argmax()\n",
    "        result[clean_expected_out].append(clean_out)\n",
    "        \n",
    "        square_error += abs(clean_expected_out-clean_out)**2\n",
    "        if clean_out == clean_expected_out:\n",
    "            perfect_accuracy += 1\n",
    "        count += 1\n",
    "        \n",
    "    #print(\"==================\")\n",
    "    for k,line in enumerate(result):\n",
    "        print(k,line)\n",
    "    \n",
    "    print(\"Mean square error {}\".format(square_error/count))\n",
    "    print(\"perfect match rate {}%\".format(round(perfect_accuracy/count*100), 2))\n",
    "    return square_error/count, perfect_accuracy/count\n",
    "mean_square_error, perfect_accuracy = manual_evaluation(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "\n",
    "# JSON\n",
    "json_model = model.to_json()\n",
    "with open(\"model_CNN_{}.json\".format(round(perfect_accuracy*100)), \"w\") as f:\n",
    "    f.write(json_model)\n",
    "# Weights\n",
    "model.save_weights(\"weight_CNN_{}.h5\".format(round(perfect_accuracy*100)))\n",
    "print(\"Model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
